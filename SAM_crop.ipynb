{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "mount_file_id": "1mDlyYB_c1pL_bQe8SJKMRSyeuSnsOSgp",
      "authorship_tag": "ABX9TyOulzXa0zPUkdG9Lw/lo8Vd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wyldescience/Segment-anything-batch-process/blob/main/SAM_crop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Segment Anything Model**\n",
        "\n",
        "Here I use the [meta](https://github.com/facebookresearch/segment-anything#model-checkpoints) SAM AI (trained on billions of images) that allows for incredible segmentation of images (even by a single click of an object see their [demo](https://segment-anything.com/demo) where you can upload your own image and cut out objects that are perfectly segmented in real time. In this instance I used this model to segment out the filter paper that contains the eggs of *Folsomia candida* to count reproductive output for a study of ageing and lifespan. The reason I wanted to crop out the small piece of filter paper from the background is that the petri dishes were reused between samples and often had stray eggs that I do not want to pick up in my egg counts during segmentation.\n",
        "After installment of the appropriate packages, the first chunk of this script is for testing/ working with single images.\n",
        "The second chunk is designed to run on batches of images and produces cropped images to an output folder of your choosing. In a number of cases, the first mask detected by the model is not the filter paper I want to retain and thus the region of interest ends up getting cropped out- the first chunk can be used and can alter the integer in the call: \"mask = predictor[0]['segmentation']\" until the correct cropping has occurred. The script also shuttles processed images to another folder of your choosing."
      ],
      "metadata": {
        "id": "E7XBzWRgLTAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segment_anything\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install pillow"
      ],
      "metadata": {
        "id": "t4ovUlo69fw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For running on an individual image file (handy for any that might not work properly when the first mask is not the filter paper (background i.e., edge of petri or other material in some cases makes the filter paper with eggs get cropped out). Try using second mask  \"mask = predictor[0]['segmentation']\" for problematic images. This cell will produce a plot of the processed and original image."
      ],
      "metadata": {
        "id": "uiuUk8thKO35"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCDFU9ST8l3f"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "MODEL_TYPE = \"vit_h\"\n",
        "CHECKPOINT_PATH = \"/content/drive/Othercomputers/ThinkPad/Desktop/opencv/sam_vit_h_4b8939.pth\"\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "# Load the SAM model\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "\n",
        "# Load the image\n",
        "image_path = \"/content/drive/Othercomputers/ThinkPad/Desktop/Folsomia candida/Data/egg count images/reprocess SAM/I1_F1_O20_CON_R5_10-09-23.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Display the loaded image using cv2_imshow\n",
        "cv2_imshow(image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Create a mask generator using SAM\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "# Convert image to RGB format for mask generation\n",
        "img_arr = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Convert image array to float32\n",
        "# img_arr = img_arr.astype(np.float32) / 255.0  # Normalize pixel values to [0, 1] range\n",
        "\n",
        "# Generate masks using the mask generator\n",
        "predictor = mask_generator.generate(img_arr)\n",
        "\n",
        "# Choose the first masks\n",
        "mask = predictor[1]['segmentation']\n",
        "\n",
        "# Remove background by turning it to white\n",
        "img_arr[mask == False] = [0, 0, 0]\n",
        "\n",
        "# Display the modified image using matplotlib\n",
        "plt.imshow(img_arr)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch script to run on multiple image files and save output to folder**"
      ],
      "metadata": {
        "id": "fcrbRiqOKyVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import shutil\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "def clear_cuda_memory():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Set environment variable to help with memory issues on GPU\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'garbage_collection_threshold:0.6,max_split_size_mb:512'\n",
        "\n",
        "MODEL_TYPE = \"vit_b\"\n",
        "CHECKPOINT_PATH = \"/content/drive/Othercomputers/ThinkPad/Desktop/Folsomia candida/Final Scripts/sam_vit_b_01ec64.pth\"\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "# Input and output directories\n",
        "input_dir = \"/content/drive/Othercomputers/ThinkPad/Desktop/Folsomia candida/Data/egg count images/reprocess SAM\"\n",
        "output_dir = \"/content/drive/Othercomputers/ThinkPad/Desktop/Folsomia candida/Data/egg count images/cropped\"\n",
        "processed_dir = \"/content/drive/Othercomputers/ThinkPad/Desktop/Folsomia candida/Data/egg count images/processed\"\n",
        "\n",
        "# List all image files in the input directory\n",
        "image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.jpg', '.png', '.tif'))]\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# Process each image file\n",
        "batch_size = 1  # Adjust the batch size based on your available memory\n",
        "points_per_batch = 4  # Adjust the points_per_batch FIXED OOM Issue but a lot slower\n",
        "for i in range(0, len(image_files), batch_size):\n",
        "    # Load the SAM model for each batch\n",
        "    sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "    sam.eval()\n",
        "\n",
        "    # Clear GPU memory before processing a new batch\n",
        "    clear_cuda_memory()\n",
        "\n",
        "    for j in range(i, min(i + batch_size, len(image_files))):\n",
        "        image_file = image_files[j]\n",
        "        image_path = os.path.join(input_dir, image_file)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        print(f\"Processing image: {image_file}\")\n",
        "\n",
        "        # Load and process the image on CPU to reduce GPU memory usage\n",
        "        img_arr = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Generate masks using the pre-created mask generator\n",
        "        with torch.no_grad():\n",
        "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "                with record_function(\"model_inference\"):\n",
        "                    mask_generator = SamAutomaticMaskGenerator(sam, points_per_batch=points_per_batch)\n",
        "                    predictor = mask_generator.generate(img_arr)\n",
        "                    mask = predictor[1]['segmentation']  # typically 0 but for erroneous crops try 1\n",
        "\n",
        "            # Print the memory profile\n",
        "            print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))\n",
        "\n",
        "        # Process the image and mask on CPU\n",
        "        img_arr[mask == False] = [0, 0, 0]\n",
        "\n",
        "        # Convert the image to PIL format\n",
        "        pil_image = Image.fromarray(img_arr)\n",
        "\n",
        "        # Set the DPI value to 300\n",
        "        dpi_value = 300\n",
        "        pil_image.info['dpi'] = (dpi_value, dpi_value)\n",
        "\n",
        "        # Save the modified image in the output directory using PIL\n",
        "        # Correct the output path to save in the specified folder\n",
        "        output_path = os.path.join(output_dir, image_file)\n",
        "        pil_image.save(output_path, dpi=(dpi_value, dpi_value))\n",
        "\n",
        "        # Move the original image to the processed originals directory\n",
        "        processed_image_path = os.path.join(processed_dir, image  _file)\n",
        "        shutil.move(image_path, processed_image_path)\n",
        "\n",
        "        # Free memory (No need to delete tensors since we're not using GPU)\n",
        "        del img_arr, mask, predictor, mask_generator\n",
        "\n",
        "    # Empty the CUDA cache after processing each batch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Processing complete. Images saved in the output directory.\")\n"
      ],
      "metadata": {
        "id": "sI90LSEMIBBn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPjxTHQEKbXH",
        "outputId": "c06f96be-f328-482b-928a-f3da61cf61dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9034570"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "vKcxf6Js0UQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "def report_gpu():\n",
        "   print(torch.cuda.list_gpu_processes())\n",
        "   gc.collect()\n",
        "   torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "bykDlg7VggzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xdy5TVWagqED"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}